\section{Opis eksperymentów}

	Eksperymenty przeprowadziliśmy w następujący sposób:
	
	\begin{figure}[h]
		\centering
		\includegraphics[width=7cm,height=8cm,keepaspectratio]{Diagram.png}
		\captionsetup{justification=centering}
		\caption{Diagram Eksperymentu}
		\label{fig:1}
	\end{figure}

	Jak widać mamy do czynienia z dwoma zmiennymi, tj. typ danych i użyta miara. Dla każdej z tych kombinacji przeprowadzamy dwuetapowy eksperyment.
	
	\subsection{Implementacja}
	
	W celu przeprowadzenia badania i wykonania potrzebnych pomiarów skorzystaliśmy z języka \textbf{Python} i obecnych dla niego bibliotek: \textbf{sklearn}, \textbf{pandas}, \textbf{matplotlib}, \textbf{seaborn}.
	
	Wybór Pythona był spowodowany łatwością i szybkością implementacji, zarówno od strony dostępnych rozwiązań(biblioteki z których korzystaliśmy miały zaimplementowane wszystkie potrzebne nam algorytmy), jak i naszych umiejętności(znaliśmy oboje najlepiej właśnie Pythona).
	
	Kod całej aplikacji dostępny jest na repozytorium \cite{2}
	
	\subsection{Walidacja krzyżowa}
	
	Trenowanie i testowanie klasyfikatorów opisanych w tej pracy odbyło się z wykorzystaniem 5x2cv, czyli 5-krotnej walidacji krzyżowej. Oznacza to że 5 razy losowo podzieliliśmy zbiór danych na dane uczące i testujące, wykonaliśmy badania i zamieniliśmy zbiory uczące i testujące miejscami.
	
	\subsection{Miary jakości}
	
	W celu określenia jakości otrzymanego wyniku, korzystaliśmy z szeregu statystyk:
	
	\begin{itemize}
		\item \textbf{accuracy} - procent poprawnych klasyfikacji. Stosunek ilości poprawnych predykcji dla danej klasyfikacji, opisana wzorem:
		\[\texttt{accuracy}(y, \hat{y}) = \frac{1}{n_\text{samples}} \sum_{i=0}^{n_\text{samples}-1} 1(\hat{y}_i = y_i)\]
		, gdzie $y$ to wartość prawdziwa, a $\hat{y}$ to wartość naszej predykcji,
		\item \textbf{precision} - umiejętność klasyfikatora do niepoprawnej klasyfikacji próbek negatywnych jako pozytywnych, najprościej opisać wzorem:
		\[ \frac{tp}{(tp + fp)}\] 
		, gdzie $tp$ to ilość prawdziwie pozytywnych predykcji(poprawnie sklasyfikowane \textit{prawdy}), a $fp$ to ilość fałszywie pozytywnych(niepoprawnie sklasyfikowanych \textit{prawd}),
		\item \textbf{recall} - umiejętność klasyfikatora do poprawnej klasyfikacji, opisana wzorem:
		\[ \frac{tp}{(tp + fn)}\]
		, gdzie $tp$ to ilość prawdziwie pozytywnych predykcji(poprawnie sklasyfikowane \textit{prawdy}), a $fn$ to ilość fałszywie negatywnych(niepoprawnie sklasyfikowanego \textit{fałszu}),
		\item \textbf{fscore} - miara bardziej skomplikowana, będąca średnią ważoną dwóch powyższych,
		\[F1 = \frac{2 * (precision * recall)}{ (precision + recall)} \] 
		\item \textbf{Confusion Matrix} - tablica błędów. Graficzna reprezentacja wszystkich wykonanych predykcji w postaci macierzy stanu faktycznego i wyliczonego przez nasz klasyfikator. Dla idealnej klasyfikacji zaznaczona jest tylko przekątna takiej macierzy.
	\end{itemize}

	W celu rozróżnieniu dwóch wyników w trakcie eksperymentu porównywaliśmy jego \textbf{fscore} i wybieraliśmy ten z większym wynikiem.